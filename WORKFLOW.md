# ðŸ¤– AI CodeScan - System Workflow Documentation

**Version**: 1.0  
**Last Updated**: May 31, 2024  
**Status**: Phase 1 Complete

---

## ðŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Authentication Workflow](#authentication-workflow)
3. [Repository Analysis Workflow](#repository-analysis-workflow)
4. [Session Management Workflow](#session-management-workflow)
5. [History Management Workflow](#history-management-workflow)
6. [CKG Operations Workflow](#ckg-operations-workflow)
7. [Code Analysis Workflow](#code-analysis-workflow)
8. [LLM Services Workflow](#llm-services-workflow)
9. [Synthesis & Reporting Workflow](#synthesis--reporting-workflow)
10. [Data Structures & Models](#data-structures--models)
11. [File Organization](#file-organization)

---

## Overview

AI CodeScan sá»­ dá»¥ng **multi-agent architecture** vá»›i **LangGraph orchestration** Ä‘á»ƒ phÃ¢n tÃ­ch code repositories. System bao gá»“m 6 major agent teams vá»›i clear data flow vÃ  responsibility separation.

### **High-Level Architecture**
```
User Input (Web UI) 
    â†“
Authentication Layer
    â†“
Session Management
    â†“
Multi-Agent Orchestrator (LangGraph)
    â†“
[Data Acquisition] â†’ [CKG Operations] â†’ [Code Analysis] â†’ [LLM Services] â†’ [Synthesis & Reporting]
    â†“
Results Display (Web UI)
    â†“
History Storage
```

---

## Authentication Workflow

### **1. User Registration/Login Flow**

#### **Entry Point**: `src/agents/interaction_tasking/auth_web_ui.py`

**Data Flow**:
```
User Input (Username/Password)
    â†“
render_login_form() [auth_web_ui.py:150-200]
    â†“
AuthService.login() [src/core/auth/auth_service.py:50-70]
    â†“
UserManager.authenticate_user() [src/core/auth/user_manager.py:80-100]
    â†“
DatabaseManager.execute_query() [src/core/auth/database_manager.py:60-80]
    â†“
Session Token Generation [auth_service.py:120-140]
    â†“
st.session_state update [auth_web_ui.py:220-240]
    â†“
Dashboard Redirect [auth_web_ui.py:250-270]
```

#### **Classes & Functions Involved**:

1. **`AuthenticatedWebUI`** (`auth_web_ui.py:50-800`)
   - `render_login_form()`: User input validation
   - `render_registration_form()`: New user creation
   - `handle_authentication()`: Process login/register

2. **`AuthService`** (`auth_service.py:20-200`)
   - `login(username, password)`: Authentication logic
   - `validate_session(token)`: Session validation
   - `logout(token)`: Session cleanup

3. **`UserManager`** (`user_manager.py:20-300`)
   - `authenticate_user()`: Password verification
   - `create_user()`: New user creation
   - `get_user_by_username()`: User lookup

4. **`DatabaseManager`** (`database_manager.py:20-150`)
   - `execute_query()`: SQL execution
   - `execute_insert()`: Data insertion
   - `execute_update()`: Data updates

### **2. Session State Management**

**Data Structure**:
```python
st.session_state = {
    'authenticated': bool,
    'user_token': str,
    'current_user': UserInfo,
    'session_id': str,
    'view_mode': 'new_session' | 'history_view',
    'selected_session': Optional[str]
}
```

---

## Repository Analysis Workflow

### **Main Analysis Flow**

#### **Entry Point**: `src/agents/interaction_tasking/auth_web_ui.py:400-500`

**Complete Data Flow**:
```
User Repository URL Input
    â†“
UserIntentParserAgent.parse_repository_request() [user_intent_parser.py:50-80]
    â†“
TaskInitiationAgent.create_repository_analysis_task() [task_initiation.py:100-130]
    â†“
LangGraph Orchestrator [project_review_graph.py:100-200]
    â†“
â”Œâ”€ Data Acquisition Team â”€â”
â”‚ GitOperationsAgent      â”‚ [git_operations.py:50-200]
â”‚ LanguageIdentifierAgentâ”‚ [language_identifier.py:50-300]
â”‚ DataPreparationAgent   â”‚ [data_preparation.py:50-400]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€ CKG Operations Team â”€â”
â”‚ CodeParserCoordinator â”‚ [code_parser_coordinator.py:50-250]
â”‚ ASTtoCKGBuilder      â”‚ [ast_to_ckg_builder.py:50-400]
â”‚ CKGQueryInterface    â”‚ [ckg_query_interface.py:50-500]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€ Code Analysis Team â”€â”
â”‚ StaticAnalysisIntegratorâ”‚ [static_analysis_integrator.py:50-300]
â”‚ ContextualQueryAgent   â”‚ [contextual_query_agent.py:50-200]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€ LLM Services Team â”€â”
â”‚ LLMGatewayAgent    â”‚ [llm_gateway_agent.py:50-200]
â”‚ OpenAIProvider     â”‚ [llm_provider_abstraction.py:100-300]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€ Synthesis & Reporting â”€â”
â”‚ FindingAggregatorAgent â”‚ [finding_aggregator.py:50-200]
â”‚ ReportGeneratorAgent   â”‚ [report_generator.py:50-400]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
PresentationAgent.display_results() [presentation.py:50-300]
    â†“
HistoryManager.save_scan_result() [history_manager.py:150-180]
```

### **Detailed Step-by-Step Breakdown**

#### **Step 1: User Input Processing**

**File**: `src/agents/interaction_tasking/user_intent_parser.py`

```python
class UserIntentParserAgent:
    def parse_repository_request(self, repo_url: str, options: Dict) -> ParsedIntent:
        # Lines 50-80
        # 1. Validate repository URL format
        # 2. Extract platform (GitHub, GitLab, BitBucket)
        # 3. Parse analysis scope from options
        # 4. Return structured ParsedIntent object
```

**Input Data**:
- `repo_url`: str - Repository URL
- `options`: Dict - UI options (language detection, test inclusion, etc.)

**Output Data**:
- `ParsedIntent` object with validated URL vÃ  analysis parameters

#### **Step 2: Task Initiation**

**File**: `src/agents/interaction_tasking/task_initiation.py`

```python
class TaskInitiationAgent:
    def create_repository_analysis_task(self, parsed_intent: ParsedIntent) -> TaskDefinition:
        # Lines 100-130
        # 1. Generate unique task ID
        # 2. Calculate priority score
        # 3. Estimate duration
        # 4. Create TaskDefinition vá»›i metadata
```

**Input Data**:
- `ParsedIntent` object

**Output Data**:
- `TaskDefinition` object vá»›i task metadata

#### **Step 3: LangGraph Orchestration**

**File**: `src/core/orchestrator/project_review_graph.py`

```python
class ProjectReviewGraph(BaseGraph):
    def build_graph(self) -> StateGraph:
        # Lines 100-200
        # 1. Define state management
        # 2. Setup agent nodes
        # 3. Configure conditional edges
        # 4. Add error handling
```

**State Management**:
```python
class CodeScanState(TypedDict):
    task_id: str
    repository_url: str
    status: TaskStatus
    current_step: str
    data_acquisition_result: Optional[ProjectDataContext]
    ckg_result: Optional[Dict]
    analysis_result: Optional[List[Finding]]
    llm_result: Optional[Dict]
    final_result: Optional[Dict]
    error_info: Optional[Dict]
```

---

## Data Acquisition Workflow

### **Git Operations**

**File**: `src/agents/data_acquisition/git_operations.py`

#### **Function Flow**:
```python
GitOperationsAgent.clone_repository(repo_url, local_path)
    â†“ [Lines 80-120]
    # 1. Validate Git URL
    # 2. Generate unique local path
    # 3. Execute git clone vá»›i error handling
    # 4. Extract repository metadata
    â†“
GitOperationsAgent.calculate_repo_size()
    â†“ [Lines 140-160]
    # Calculate size, file count
    â†“
GitOperationsAgent.detect_basic_languages()
    â†“ [Lines 180-200]
    # Basic language detection tá»« extensions
```

**Output**: `RepositoryInfo` object

### **Language Identification**

**File**: `src/agents/data_acquisition/language_identifier.py`

#### **Function Flow**:
```python
LanguageIdentifierAgent.identify_language(project_path)
    â†“ [Lines 100-150]
    LanguageIdentifierAgent.analyze_file_extensions()
        â†“ [Lines 180-220]
        # Count files by extension
    â†“
    LanguageIdentifierAgent.analyze_config_files()
        â†“ [Lines 240-280]
        # Detect frameworks tá»« config files
    â†“
    LanguageIdentifierAgent.detect_frameworks()
        â†“ [Lines 300-350]
        # Pattern matching cho frameworks
    â†“
    LanguageIdentifierAgent.calculate_confidence()
        â†“ [Lines 380-400]
        # Score calculation based on evidence
```

**Output**: `ProjectLanguageProfile` object

### **Data Preparation**

**File**: `src/agents/data_acquisition/data_preparation.py`

#### **Function Flow**:
```python
DataPreparationAgent.prepare_project_context(repo_url, local_path, language_profile)
    â†“ [Lines 150-200]
    DataPreparationAgent.analyze_directory_structure()
        â†“ [Lines 220-260]
        # Build directory tree
    â†“
    DataPreparationAgent.analyze_files()
        â†“ [Lines 280-350]
        # Process individual files
        â†“
        DataPreparationAgent.extract_python_metadata()
            â†“ [Lines 400-450]
            # Parse Python config files
    â†“
    DataPreparationAgent.create_project_data_context()
        â†“ [Lines 500-550]
        # Aggregate all data
```

**Output**: `ProjectDataContext` object

---

## CKG Operations Workflow

### **Code Parsing Coordination**

**File**: `src/agents/ckg_operations/code_parser_coordinator.py`

#### **Function Flow**:
```python
CodeParserCoordinatorAgent.parse_python_project(project_path)
    â†“ [Lines 80-120]
    # 1. Discover Python files
    # 2. Generate AST for each file
    # 3. Extract code elements
    # 4. Aggregate parse results
```

### **AST to CKG Building**

**File**: `src/agents/ckg_operations/ast_to_ckg_builder.py`

#### **Function Flow**:
```python
ASTtoCKGBuilderAgent.build_ckg_from_project(parse_results)
    â†“ [Lines 100-150]
    ASTtoCKGBuilderAgent.process_ast_nodes()
        â†“ [Lines 200-250]
        # Convert AST nodes thÃ nh CKG nodes
    â†“
    ASTtoCKGBuilderAgent.extract_relationships()
        â†“ [Lines 300-350]
        # Extract calls, imports, inheritance
    â†“
    ASTtoCKGBuilderAgent.generate_cypher_queries()
        â†“ [Lines 400-450]
        # Create Neo4j insertion queries
    â†“
    ASTtoCKGBuilderAgent.save_to_neo4j()
        â†“ [Lines 500-550]
        # Execute queries vá»›i batch processing
```

**Output**: CKG stored in Neo4j database

---

## Code Analysis Workflow

### **Static Analysis Integration**

**File**: `src/agents/code_analysis/static_analysis_integrator.py`

#### **Function Flow**:
```python
StaticAnalysisIntegratorAgent.run_comprehensive_analysis(project_path)
    â†“ [Lines 80-100]
    StaticAnalysisIntegratorAgent.run_flake8()
        â†“ [Lines 150-200]
        # Execute flake8, parse output
    â†“
    StaticAnalysisIntegratorAgent.run_pylint()
        â†“ [Lines 250-300]
        # Execute pylint, parse output
    â†“
    StaticAnalysisIntegratorAgent.run_mypy()
        â†“ [Lines 350-400]
        # Execute mypy, parse output
    â†“
    StaticAnalysisIntegratorAgent.aggregate_findings()
        â†“ [Lines 450-500]
        # Combine results tá»« all tools
```

### **Contextual Query Analysis**

**File**: `src/agents/code_analysis/contextual_query_agent.py`

#### **Function Flow**:
```python
ContextualQueryAgent.analyze_findings_with_context(findings, ckg_interface)
    â†“ [Lines 100-150]
    # 1. Enrich findings vá»›i CKG context
    # 2. Calculate impact scores
    # 3. Generate contextual recommendations
    # 4. Detect related findings
```

**Output**: Enhanced `ContextualFinding` objects

---

## LLM Services Workflow

### **LLM Gateway Agent**

**File**: `src/agents/llm_services/llm_gateway_agent.py`

#### **Function Flow**:
```python
LLMGatewayAgent.explain_code_finding(finding, context)
    â†“ [Lines 100-120]
    LLMGatewayAgent.select_provider()
        â†“ [Lines 150-170]
        # Choose available LLM provider
    â†“
    LLMProvider.send_request(llm_request)
        â†“ [Lines 200-250] [llm_provider_abstraction.py]
        # Send request to OpenAI or Mock provider
    â†“
    LLMGatewayAgent.process_response()
        â†“ [Lines 280-300]
        # Parse vÃ  validate response
```

**Output**: `LLMResponse` vá»›i code explanations

---

## Synthesis & Reporting Workflow

### **Finding Aggregation**

**File**: `src/agents/synthesis_reporting/finding_aggregator.py`

#### **Function Flow**:
```python
FindingAggregatorAgent.aggregate_findings(findings_list, strategy)
    â†“ [Lines 100-150]
    FindingAggregatorAgent.deduplicate_findings()
        â†“ [Lines 200-250]
        # Remove similar findings
    â†“
    FindingAggregatorAgent.calculate_priority_scores()
        â†“ [Lines 300-350]
        # Score findings by importance
    â†“
    FindingAggregatorAgent.generate_statistics()
        â†“ [Lines 400-450]
        # Calculate summary metrics
```

### **Report Generation**

**File**: `src/agents/synthesis_reporting/report_generator.py`

#### **Function Flow**:
```python
ReportGeneratorAgent.generate_report(aggregated_findings, format)
    â†“ [Lines 100-120]
    ReportGeneratorAgent.select_format_generator()
        â†“ [Lines 150-180]
        # Choose format (TEXT, JSON, HTML, CSV, MARKDOWN)
    â†“
    ReportGeneratorAgent.generate_content()
        â†“ [Lines 200-400]
        # Generate formatted report
        â†“
        ReportGeneratorAgent.generate_executive_summary()
            â†“ [Lines 450-500]
            # Create executive summary
```

**Output**: Formatted reports in multiple formats

---

## Session Management Workflow

### **Session Creation & Tracking**

**File**: `src/core/session_manager/history_manager.py`

#### **Function Flow**:
```python
HistoryManager.create_session(session_type, user_id)
    â†“ [Lines 100-130]
    # 1. Generate unique session ID
    # 2. Initialize session metadata
    # 3. Save to JSON storage
    â†“
HistoryManager.save_scan_result(session_id, scan_result)
    â†“ [Lines 180-210]
    # 1. Validate session exists
    # 2. Save scan result data
    # 3. Update session status
```

### **Session State Management**

**Data Flow**:
```
Session Creation
    â†“
Real-time Status Updates
    â†“
Result Storage
    â†“
History Persistence
    â†“
Cross-session Retrieval
```

---

## History Management Workflow

### **History Storage & Retrieval**

**File**: `src/core/session_manager/history_manager.py`

#### **Storage Structure**:
```
logs/history/
â”œâ”€â”€ sessions/           # Session metadata
â”‚   â””â”€â”€ {session_id}.json
â”œâ”€â”€ scans/             # Scan results
â”‚   â””â”€â”€ {session_id}_scan.json
â””â”€â”€ chats/             # Chat messages
    â””â”€â”€ {session_id}_chat.json
```

#### **Function Flow**:
```python
HistoryManager.get_all_sessions(user_id, filters)
    â†“ [Lines 250-300]
    # 1. Load session metadata files
    # 2. Apply filters (session_type, status, date_range)
    # 3. Sort by created_at
    # 4. Return SessionHistory objects
    â†“
HistoryManager.get_session(session_id)
    â†“ [Lines 320-350]
    # 1. Load session metadata
    # 2. Load associated scan results
    # 3. Load chat history
    # 4. Return complete session data
```

---

## Data Structures & Models

### **Core Data Models**

#### **Authentication Models** (`src/core/auth/models.py`)
```python
@dataclass
class UserInfo:
    user_id: str
    username: str
    email: str
    role: UserRole
    created_at: datetime
    last_login: Optional[datetime]

@dataclass
class SessionInfo:
    token: str
    user_id: str
    created_at: datetime
    expires_at: datetime
    last_accessed: datetime
```

#### **Task & Session Models** (`src/core/session_manager/`)
```python
@dataclass
class TaskDefinition:
    task_id: str
    task_type: TaskType
    repository_url: str
    analysis_scope: Dict[str, Any]
    priority: int
    estimated_duration: int
    created_at: datetime

class SessionType(Enum):
    REPOSITORY_ANALYSIS = "repository_analysis"
    PR_REVIEW = "pr_review"  
    CODE_QNA = "code_qna"

@dataclass
class SessionHistory:
    session_id: str
    session_type: SessionType
    status: SessionStatus
    user_id: str
    created_at: datetime
    updated_at: datetime
    metadata: Dict[str, Any]
```

#### **Analysis Result Models**
```python
@dataclass
class Finding:
    file_path: str
    line_number: int
    severity: SeverityLevel
    finding_type: FindingType
    message: str
    rule: str
    source_tool: str

@dataclass
class ProjectDataContext:
    repository_info: RepositoryInfo
    language_profile: ProjectLanguageProfile
    directory_structure: DirectoryStructure
    file_analysis: List[FileInfo]
    project_metadata: ProjectMetadata
```

---

## File Organization

### **Source Code Structure**
```
src/
â”œâ”€â”€ agents/                          # Multi-agent implementation
â”‚   â”œâ”€â”€ interaction_tasking/         # Web UI & User interaction
â”‚   â”‚   â”œâ”€â”€ auth_web_ui.py          # Main authenticated web interface
â”‚   â”‚   â”œâ”€â”€ user_intent_parser.py   # User input processing
â”‚   â”‚   â”œâ”€â”€ task_initiation.py      # Task creation
â”‚   â”‚   â”œâ”€â”€ dialog_manager.py       # UI state management
â”‚   â”‚   â””â”€â”€ presentation.py         # Results display
â”‚   â”œâ”€â”€ data_acquisition/           # Repository data gathering
â”‚   â”‚   â”œâ”€â”€ git_operations.py       # Git clone & operations
â”‚   â”‚   â”œâ”€â”€ language_identifier.py  # Language detection
â”‚   â”‚   â””â”€â”€ data_preparation.py     # Project context building
â”‚   â”œâ”€â”€ ckg_operations/             # Code Knowledge Graph
â”‚   â”‚   â”œâ”€â”€ ckg_schema.py           # CKG schema definition
â”‚   â”‚   â”œâ”€â”€ code_parser_coordinator.py # AST parsing
â”‚   â”‚   â”œâ”€â”€ ast_to_ckg_builder.py   # AST to CKG conversion
â”‚   â”‚   â””â”€â”€ ckg_query_interface.py  # Neo4j query interface
â”‚   â”œâ”€â”€ code_analysis/              # Static analysis
â”‚   â”‚   â”œâ”€â”€ static_analysis_integrator.py # Linter integration
â”‚   â”‚   â””â”€â”€ contextual_query_agent.py     # CKG-enhanced analysis
â”‚   â”œâ”€â”€ llm_services/               # LLM integration
â”‚   â”‚   â”œâ”€â”€ llm_provider_abstraction.py # Provider interface
â”‚   â”‚   â””â”€â”€ llm_gateway_agent.py         # LLM orchestration
â”‚   â””â”€â”€ synthesis_reporting/        # Results synthesis
â”‚       â”œâ”€â”€ finding_aggregator.py   # Finding deduplication
â”‚       â””â”€â”€ report_generator.py     # Multi-format reports
â”œâ”€â”€ core/                           # Core infrastructure
â”‚   â”œâ”€â”€ orchestrator/              # LangGraph workflow
â”‚   â”‚   â”œâ”€â”€ base_graph.py          # Abstract base graph
â”‚   â”‚   â””â”€â”€ project_review_graph.py # Concrete implementation
â”‚   â”œâ”€â”€ auth/                      # Authentication system
â”‚   â”‚   â”œâ”€â”€ database_manager.py    # Database operations
â”‚   â”‚   â”œâ”€â”€ user_manager.py        # User management
â”‚   â”‚   â”œâ”€â”€ auth_service.py        # Session management
â”‚   â”‚   â””â”€â”€ models.py              # Auth data models
â”‚   â”œâ”€â”€ session_manager/           # Session & history
â”‚   â”‚   â””â”€â”€ history_manager.py     # Session persistence
â”‚   â””â”€â”€ logging/                   # Debug logging
â”‚       â””â”€â”€ debug_logger.py        # Comprehensive logging
â””â”€â”€ main.py                        # Application entry point
```

### **Data Storage Structure**
```
data/
â””â”€â”€ ai_codescan.db                 # SQLite authentication database

logs/
â”œâ”€â”€ debug/                         # Debug logs per session
â”‚   â”œâ”€â”€ debug_{session_id}.log     # Main debug log
â”‚   â”œâ”€â”€ trace_{session_id}.log     # Function call traces
â”‚   â””â”€â”€ summary_{session_id}.json  # Session summary
â””â”€â”€ history/                       # Session persistence
    â”œâ”€â”€ sessions/                  # Session metadata
    â”œâ”€â”€ scans/                     # Scan results  
    â””â”€â”€ chats/                     # Chat history

temp_repos/                        # Temporary repository storage
â””â”€â”€ {unique_repo_names}/           # Cloned repositories
```

---

## Integration Points

### **Database Connections**
- **Neo4j**: CKG storage (port 7687)
- **SQLite**: Authentication (data/ai_codescan.db)
- **Redis**: Session caching (port 6379)
- **File System**: History persistence (logs/history/)

### **External Services**
- **OpenAI API**: LLM services
- **Git Repositories**: GitHub, GitLab, BitBucket
- **Static Analysis Tools**: flake8, pylint, mypy

### **Docker Services**
- **ai-codescan**: Main application (port 8501)
- **neo4j**: Graph database (ports 7474, 7687)
- **redis**: Caching service (port 6379)
- **portainer**: Container management (port 9000)

---

## Error Handling & Recovery

### **Error Propagation Flow**
```
Agent Error
    â†“
LangGraph Error Handler [project_review_graph.py:300-350]
    â†“
State Update vá»›i error info
    â†“
UI Error Display [auth_web_ui.py:600-650]
    â†“
Session Error Logging [history_manager.py:400-430]
```

### **Recovery Mechanisms**
1. **Graceful Degradation**: Continue workflow without failed component
2. **Retry Logic**: Automatic retry vá»›i exponential backoff
3. **Fallback Systems**: Alternative providers cho LLM services
4. **State Persistence**: Resume tá»« last checkpoint

---

*Generated: May 31, 2024*  
*AI CodeScan Phase 1 Complete*  
*Version: 1.0* 